{"cells":[{"cell_type":"code","source":"import random\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\nimport gc\r\nimport re\r\nfrom torch import nn\r\nfrom keras.preprocessing.text import Tokenizer\r\n\r\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\r\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU\r\n\r\nfrom keras.models import Model\r\nfrom keras.preprocessing.sequence import pad_sequences\r\n\r\nfrom fastai.train import Learner\r\nfrom fastai.train import DataBunch\r\nfrom fastai.callbacks import TrainingPhase, GeneralScheduler\r\nimport torch\r\nfrom torch.utils import data\r\nfrom torch.nn import functional as F\r\nimport sys\r\n\r\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\r\nsys.path.append(package_dir)\r\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\r\nfrom pytorch_pretrained_bert import BertConfig\r\n\r\nEMB_MAX_FEAT = 300\r\nMAX_LEN = 220\r\nmax_features = 400000\r\nbatch_size = 512\r\nNUM_EPOCHS = 4\r\nLSTM_UNITS = 128\r\nDENSE_HIDDEN_UNITS = 512\r\nNUM_MODELS = 1\r\n\r\nEMB_PATHS = [\r\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\r\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\r\n]\r\n\r\nJIGSAW_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\r\n\r\n\r\ndef C_trans_to_E():\r\n    E_pun = 'BEFIKLMOQSTWZBCFGJKLMPVWXZCILOABCDFGHJKLMNOPQRSUVWXYABCDEHIJKLMOPRTWYBJKMVXZ012345678901234567FWY,;.!?:$ []()<>\"\"\\'\\'AAAAAAAABBCCDDDDEEEEEEEEFFGGGHHHHIIIIIIJKLLLMNNNNNOOOOOOOPPRRRSSSSTTTTTTUUUUVWWYYYZPABBDEGHIJKLMNOPRTUWabdegkmnNoptuwvhjrwyxylsx\\'\\'    ' + 'ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ'\r\n    C_pun = 'ð—•ð—˜ð—™ð—œð—žð—Ÿð— ð—¢ð—¤ð—¦ð—§ð—ªð—­ð—¯ð—°ð—³ð—´ð—·ð—¸ð—¹ð—ºð—½ð˜ƒð˜„ð˜…ð˜‡ð˜Šð˜ð˜“ð˜–ð˜¢ð˜£ð˜¤ð˜¥ð˜§ð˜¨ð˜©ð˜«ð˜¬ð˜­ð˜®ð˜¯ð˜°ð˜±ð˜²ð˜³ð˜´ð˜¶ð˜·ð˜¸ð˜¹ð˜ºð˜¼ð˜½ð˜¾ð˜¿ð™€ð™ƒð™„ð™…ð™†ð™‡ð™ˆð™Šð™‹ð™ð™ð™’ð™”ð™—ð™Ÿð™ ð™¢ð™«ð™­ð™¯ðŸŽðŸðŸðŸ‘ðŸ’ðŸ“ðŸ”ðŸ•ðŸ–ðŸ—ðŸ¬ðŸ­ðŸ®ðŸ¯ðŸ°ðŸ±ðŸ²ðŸ³ðŸ‡«ðŸ‡¼ðŸ‡¾ï¼Œï¼›ã€‚ï¼ï¼Ÿï¼šï¿¥â€”ã€ã€‘ï¼ˆï¼‰ã€Šã€‹â€œâ€â€˜â€™ð™–ð—®Ã¤Ð°ÄÃ Ã¡á´€á´ƒÊ™á´„ð™˜á´…ð™™ð—±á´†ð™šð–Šð˜¦ð—²Ã©Ã¨Ãªá´‡Ò“ð™›Ê›É¢ð™œÊœÐ½ð—µð™Ã­ð—¶ð˜ªð™žÃ¯Éªá´Šá´‹ð™¡ÊŸá´Œá´á´Žð™£ð—»Ã±ð–“á´Ð¾ð–”ð—¼ð™¤Ã¶Ã³á´˜ð™¥Ê€ð™§ð—¿ð˜€Ñ•Å›ð™¨á´›Ñ‚ð–™ð˜µð˜ð™©Å«ð™ªð˜‚á´œá´ á´¡ð™¬Êð™®ð˜†á´¢á´©á´¬á´®á´¯á´°á´±á´³á´´á´µá´¶á´·á´¸á´¹á´ºÉ´á´¼á´¾á´¿áµ€áµáµ‚áµƒáµ‡áµˆáµ‰áµáµáµáµ‘áµ’áµ–áµ—áµ˜áµšáµ›Ê°Ê²Ê³Ê·Ê¸ËŸË Ë¡Ë¢Ë£Â´`â€¦-_â€¢' + 'ð€ðð‚ðƒð„ð…ð†ð‡ðˆð‰ðŠð‹ðŒððŽððð‘ð’ð“ð”ð•ð–ð—ð˜ð™ðšð›ðœððžðŸð ð¡ð¢ð£ð¤ð¥ð¦ð§ð¨ð©ðªð«ð¬ð­ð®ð¯ð°ð±ð²ð³ð´ðµð¶ð·ð¸ð¹ðºð»ð¼ð½ð¾ð¿ð‘€ð‘ð‘‚ð‘ƒð‘„ð‘…ð‘†ð‘‡ð‘ˆð‘‰ð‘Šð‘‹ð‘Œð‘ð‘Žð‘ð‘ð‘‘ð‘’ð‘“ð‘”ð‘–ð‘—ð‘˜ð‘™ð‘šð‘›ð‘œð‘ð‘žð‘Ÿð‘ ð‘¡ð‘¢ð‘£ð‘¤ð‘¥ð‘¦ð‘§ð‘¨ð‘©ð‘ªð‘«ð‘¬ð‘­ð‘®ð‘¯ð‘°ð‘±ð‘²ð‘³ð‘´ð‘µð‘¶ð‘·ð‘¸ð‘¹ð‘ºð‘»ð‘¼ð‘½ð‘¾ð‘¿ð’€ð’ð’‚ð’ƒð’„ð’…ð’†ð’‡ð’ˆð’‰ð’Šð’‹ð’Œð’ð’Žð’ð’ð’‘ð’’ð’“ð’”ð’•ð’–ð’—ð’˜ð’™ð’šð’›'\r\n    table = {ord(f): ord(t) for f, t in zip(C_pun, E_pun)}\r\n    return table\r\n\r\n\r\ndef char_to_word(string):\r\n    char_list = string.split()\r\n    new_word_list = []\r\n    new_word = \"\"\r\n    for i in range(len(char_list)):\r\n        if len(char_list[i]) == 1 and char_list[i] in list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\r\n            new_word += char_list[i]\r\n        else:\r\n            if len(new_word) > 3:\r\n                new_word_list.append(new_word)\r\n            new_word = \"\"\r\n    \r\n    if len(new_word_list) > 0:\r\n        return new_word_list\r\n    else:\r\n        return np.nan\r\n\r\n\r\ndef build_doc_vocab(texts):\r\n    sentences = texts.apply(lambda x: set(x.split())).values\r\n    vocab = {}\r\n    for sentence in sentences:\r\n        for word in sentence:\r\n            try:\r\n                vocab[word] += 1\r\n            except KeyError:\r\n                vocab[word] = 1\r\n    return vocab\r\n\r\n\r\ndef build_vocab(texts):\r\n    sentences = texts.apply(lambda x: x.split()).values\r\n    vocab = {}\r\n    for sentence in sentences:\r\n        for word in sentence:\r\n            try:\r\n                vocab[word] += 1\r\n            except KeyError:\r\n                vocab[word] = 1\r\n    return vocab\r\n\r\n\r\ndef word_split(key, ngram, embeddings_index, vocab1):\r\n    new_word_list = []\r\n    length = len(key)\r\n    if ngram == 2:\r\n        for i in range(1, length - 1):\r\n            if key[:i] in vocab1 and key[i:] in vocab1:\r\n                new_word_list.append(\" \".join([key[:i], key[i:]]))\r\n    elif ngram == 3:\r\n        for i in range(1, length - 2):\r\n            for j in range(1, length - i - 1):\r\n                if key[:i] in vocab1 and key[i:i + j] in vocab1 and key[i + j:] in vocab1:\r\n                    new_word_list.append(\" \".join([key[:i], key[i:i + j], key[i + j:]]))\r\n    \r\n    elif ngram == 4:\r\n        for i in range(1, length - 3):\r\n            for j in range(1, length - i - 2):\r\n                for k in range(1, length - i - j - 1):\r\n                    if key[:i] in vocab1 and key[i:i + j] in vocab1 and key[i + j:i + j + k] in vocab1 and key[\r\n                                                                                                           i + j + k:] in vocab1:\r\n                        new_word_list.append(\" \".join([key[:i], key[i:i + j], key[i + j:i + j + k], key[i + j + k:]]))\r\n    if len(new_word_list) > 0:\r\n        return new_word_list\r\n    else:\r\n        return None\r\n\r\n\r\ndef build_vocab_ngram(texts, ngram, vocab1):\r\n    sentences = texts.apply(lambda x: x.split()).values\r\n    vocab = {}\r\n    for sentence in sentences:\r\n        if len(sentence) > ngram - 1:\r\n            for i in range(len(sentence) - 1):\r\n                if len([word for word in sentence[i:i + ngram] if word in vocab1]) > 0:\r\n                    try:\r\n                        vocab[\" \".join(sentence[i:i + ngram])] += 1\r\n                    except KeyError:\r\n                        vocab[\" \".join(sentence[i:i + ngram])] = 1\r\n    return vocab\r\n\r\n\r\ndef long_word_dict(comment_text, embeddings_index, ngram):\r\n    vocab = build_vocab(comment_text)\r\n    vocab = pd.Series(vocab).reset_index()\r\n    vocab.columns = [\"key\", \"cnt\"]\r\n    vocab1 = vocab[(vocab.cnt > 10) & (vocab.key.isin(embeddings_index))]\r\n    vocab1 = vocab1.set_index(\"key\").cnt.to_dict()\r\n    vocab = vocab[\r\n        (~vocab.key.isin(embeddings_index)) & (~vocab.key.str.contains(\"[^a-z0-9]\")) & (vocab.key.str.len() > 5)]\r\n    vocab_ngram = build_vocab_ngram(comment_text, ngram, vocab1)\r\n    vocab[\"new_word_list\"] = vocab.key.map(lambda x: word_split(x, ngram, embeddings_index, vocab1))\r\n    \r\n    def method_key(word):\r\n        if word in vocab_ngram:\r\n            return vocab_ngram[word]\r\n        else:\r\n            return 0\r\n    \r\n    vocab = vocab[vocab.new_word_list.notnull()]\r\n    vocab[\"new_word\"] = vocab[\"new_word_list\"].map(lambda x: max(x, key=method_key))\r\n    vocab = vocab.drop(\"new_word_list\", axis=1)\r\n    vocab[\"new_word_cnt\"] = vocab[\"new_word\"].map(method_key)\r\n    vocab = vocab[vocab.new_word_cnt > 1]\r\n    return vocab.set_index(\"key\").new_word.to_dict()\r\n\r\n\r\ndef long_word_dict_new(df, embeddings_index):\r\n    vocab = build_vocab(df.comment_text.str.lower())\r\n    vocab = pd.Series(vocab).reset_index()\r\n    vocab.columns = [\"key\", \"cnt\"]\r\n    vocab1 = vocab[(vocab.cnt > 10) & (vocab.key.isin(embeddings_index))]\r\n    vocab1 = vocab1.set_index(\"key\").cnt.to_dict()\r\n    vocab = vocab[\r\n        (~vocab.key.isin(embeddings_index)) & (~vocab.key.str.contains(\"[^a-z0-9]\")) & (vocab.key.str.len() > 5)]\r\n    for ngram in range(2, 5):\r\n        vocab_ngram = build_vocab_ngram(df.comment_text[df.target > 0].str.lower(), ngram, vocab1)\r\n        vocab[\"new_word_list{}\".format(ngram)] = vocab.key.map(lambda x: word_split(x, ngram, embeddings_index, vocab1))\r\n        \r\n        def method_key(word):\r\n            if word in vocab_ngram:\r\n                return vocab_ngram[word]\r\n            else:\r\n                return 0\r\n        \r\n        vocab[\"new_word{}\".format(ngram)] = vocab[\"new_word_list{}\".format(ngram)].map(\r\n            lambda x: max(x, key=method_key) if type(x) == list else None)\r\n        vocab = vocab.drop(\"new_word_list{}\".format(ngram), axis=1)\r\n        vocab[\"new_word_cnt{}\".format(ngram)] = vocab[\"new_word{}\".format(ngram)].map(method_key)\r\n        del vocab_ngram\r\n        gc.collect()\r\n    vocab = vocab[(vocab[[\"new_word_cnt2\", \"new_word_cnt3\", \"new_word_cnt4\"]] > 1).any(1)]\r\n    vocab[\"new_word_index\"] = vocab[[\"new_word_cnt2\", \"new_word_cnt3\", \"new_word_cnt4\"]].apply(\r\n        lambda x: \"new_word\" + x.idxmax()[-1], axis=1)\r\n    vocab[\"new_word\"] = vocab.apply(lambda x: x[x.new_word_index], axis=1)\r\n    return vocab.set_index(\"key\").new_word.to_dict()\r\n\r\n\r\ndef word_regex(series, embeddings_index):\r\n    string = series.comment_text\r\n    for word in series.char_word:\r\n        if word.lower() in embeddings_index:\r\n            regex = \"\"\r\n            for char_index in range(len(word)):\r\n                regex += word[char_index]\r\n                if char_index < len(word) - 1:\r\n                    regex += \"[^a-zA-Z]+\"\r\n            string = re.sub(regex, word.lower(), string)\r\n    return string\r\n\r\n\r\ndef clean_punct(df, punct):\r\n    for char in punct:\r\n        regex = \"\\\\\" + char\r\n        new_char = \" \" + char + \" \"\r\n        df[\"comment_text\"] = df.comment_text.str.replace(regex, new_char)\r\n    return df\r\n\r\n\r\ndef long_word(key, emb_idx):\r\n    length = len(key)\r\n    for i in range(2, max(10, int(length / 2))):\r\n        if length % i == 0 and key == \"\".join((list(key[:i]) * int(length / i))) and key[:i] in emb_idx:\r\n            return key[:i], emb_idx[key[:i]]\r\n    return key, None\r\n\r\n\r\ndef word_replace(text, replace_dict):\r\n    text = ' '.join(\r\n        [replace_dict[t] if t in replace_dict else replace_dict[t.lower()] if t.lower() in replace_dict else t for t in\r\n         text.split()])\r\n    return text\r\n\r\n\r\ndef text_clean(text):\r\n    contraction_patterns = [(r'(\\w+)\\'ll', '\\g<1> will'),\r\n                            (r'(\\w+)n\\'t', '\\g<1> not'),\r\n                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'),\r\n                            (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would')]\r\n    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\r\n    for (pattern, repl) in patterns:\r\n        (text, count) = re.subn(pattern, repl, text)\r\n    return text\r\n\r\n\r\ndef build_char_count(texts):\r\n    sentences = texts.apply(list).values\r\n    vocab = {}\r\n    for sentence in sentences:\r\n        for char in sentence:\r\n            try:\r\n                vocab[char] += 1\r\n            except KeyError:\r\n                vocab[char] = 1\r\n    return vocab\r\n\r\n\r\ndef char_clean(text, table):\r\n    return text.translate(table)\r\n\r\n\r\ndef special_char_dict(df, embeddings_index):\r\n    emb_iddx = set()\r\n    for word in embeddings_index:\r\n        emb_iddx = emb_iddx | set(list(word))\r\n    vocab = build_char_count(df.comment_text)\r\n    char_dict = {ord(char): \"\" for char in vocab.keys() if char not in emb_iddx and char not in list(\"\\r\\n\\t \")}\r\n    return char_dict\r\n\r\n\r\ndef multiple_replace(text, adict):\r\n    rx = re.compile('|'.join(map(re.escape, adict)))\r\n    \r\n    def one_xlat(match):\r\n        return adict[match.group(0)]\r\n    \r\n    return rx.sub(one_xlat, text)\r\n\r\n\r\ndef word_clean(word_list, vocab):\r\n    word_dict = {}\r\n    for word in word_list:\r\n        regex = re.sub(\"\\^|\\$|\\*|\\@|\\#\", \".{0,1}\", word)\r\n        vocabx = vocab[vocab.index.map(lambda x: True if re.fullmatch(regex, x) else False)]\r\n        if len(vocabx) > 0:\r\n            word_dict[word] = vocabx.idxmax()\r\n    return word_dict\r\n\r\n\r\ndef char_spell_check(word):\r\n    char_list = list(word.lower())\r\n    count = 0\r\n    word_check = False\r\n    char_set = []\r\n    for char_index in range(len(char_list) - 1):\r\n        if char_list[char_index] == char_list[char_index + 1]:\r\n            count += 1\r\n        else:\r\n            if count > 2:\r\n                char_set.append(char_list[char_index])\r\n            if count > 3:\r\n                word_check = True\r\n            count = 0\r\n    if count > 2:\r\n        char_set.append(char_list[-1])\r\n    if count > 3:\r\n        word_check = True\r\n    if word_check:\r\n        return set(char_set)\r\n    else:\r\n        return np.nan\r\n\r\n\r\ndef spell_dict_create(vocab1, vocab):\r\n    spell_dict = {}\r\n    for index in vocab1.index:\r\n        word = vocab1.word[index]\r\n        regex = word.lower()\r\n        for char in vocab1.char_set[index]:\r\n            regex = re.sub(\"{}+\".format(char), \"{}+\".format(char), regex)\r\n        vocabx = vocab[vocab.index.map(lambda x: True if re.fullmatch(regex, x) else False)]\r\n        if len(vocabx) > 0:\r\n            spell_dict[word] = vocabx.idxmax()\r\n    return spell_dict\r\n\r\n\r\ndef build_vocab_ngram_new(texts, ngram):\r\n    sentences = texts.apply(lambda x: x.split()).values\r\n    vocab = {}\r\n    for sentence in sentences:\r\n        if len(sentence) > ngram - 1:\r\n            for i in range(len(sentence) - 1):\r\n                try:\r\n                    vocab[\" \".join(sentence[i:i + ngram])] += 1\r\n                except KeyError:\r\n                    vocab[\" \".join(sentence[i:i + ngram])] = 1\r\n    return vocab\r\n\r\n\r\ndef word_merge_dict(df, embeddings_index):\r\n    vocabp = build_vocab_ngram_new(df.comment_text, 2)\r\n    vocabp = pd.Series(vocabp).reset_index()\r\n    vocabp.columns = [\"word2\", \"cnt\"]\r\n    vocabp = vocabp[~vocabp.word2.str.contains(\"[^a-zA-Z ]\")]\r\n    vocabp2 = vocabp.word2.str.split(\" \", expand=True)\r\n    vocabp2.columns = [\"word0\", \"word1\"]\r\n    vocabp3 = (vocabp2.word0.str.lower().isin(embeddings_index)) & (vocabp2.word1.str.lower().isin(embeddings_index))\r\n    vocabp4 = vocabp[~vocabp3]\r\n    vocabp4[\"word3\"] = vocabp4.word2.str.replace(\" \", \"\")\r\n    vocabp5 = vocabp4[vocabp4.word3.str.lower().isin(embeddings_index)]\r\n    word_dict = vocabp5.set_index(\"word2\").word3.to_dict()\r\n    word_list = vocabp[(vocabp.word2.isin(word_dict.keys())) & (vocabp.cnt > 1)].word2.values\r\n    return {key: word_dict[key] for key in word_list}\r\n\r\n\r\ndef convert_lines(example, max_seq_length, tokenizer):\r\n    max_seq_length -= 2\r\n    all_tokens = []\r\n    longer = 0\r\n    for text in example:\r\n        tokens_a = tokenizer.tokenize(text)\r\n        if len(tokens_a) > max_seq_length:\r\n            tokens_a = tokens_a[:max_seq_length]\r\n            longer += 1\r\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (\r\n            max_seq_length - len(tokens_a))\r\n        all_tokens.append(one_token)\r\n    return np.array(all_tokens)\r\n\r\n\r\ndef bert_predict(test_df, BERT_MODEL_PATH, bert_path):\r\n    bert_config = BertConfig(bert_path + 'bert_config.json')\r\n    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\r\n    \r\n    device = torch.device('cuda')\r\n    model = BertForSequenceClassification(bert_config, num_labels=8)\r\n    model.load_state_dict(torch.load(bert_path + \"bert_pytorch.bin\"))\r\n    model.to(device)\r\n    for param in model.parameters():\r\n        param.requires_grad = False\r\n    model.eval()\r\n    X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), 220, tokenizer)\r\n    test_preds = np.zeros((len(X_test)))\r\n    test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\r\n    test_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\r\n    for i, (x_batch,) in enumerate(test_loader):\r\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\r\n        test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\r\n    \r\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\r\n    return test_pred\r\n\r\n\r\ndef load_data_and_clean(debug=False):\r\n    embeddings_index = load_embeddings1(EMB_PATHS)\r\n    train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\r\n    test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\r\n    if debug:\r\n        train = train.iloc[:10000]\r\n        test = test.iloc[:10000]\r\n    df = pd.concat([train, test], ignore_index=True)[[\"comment_text\", \"target\"]]\r\n    del train, test\r\n    gc.collect()\r\n    df[\"comment_text\"] = df.comment_text.fillna(\"\")\r\n    translate_dict = C_trans_to_E()\r\n    df[\"comment_text\"] = df.comment_text.map(lambda x: x.translate(translate_dict))\r\n    df[\"comment_text\"] = df.comment_text.str.replace(\"Â·\", \"\").str.replace(\"\\xad\", \"\")\r\n    df[\"comment_text\"] = df.comment_text.replace(\"(U|u)\\.(S|s)\\.(A|a)\", \"USA\")\r\n    df[\"comment_text\"] = df.comment_text.map(lambda x: re.sub(\"((ht|f)tps?):\\/\\/[^\\s]*\", \"http \", x))\r\n    df[\"comment_text\"] = df.comment_text.map(lambda x: re.sub(\"[\\u0800-\\u4e00\\uAC00-\\uD7A3\\u4E00-\\u9FA5]\", \"\", x))\r\n    df = clean_punct(df, \"?!.,()/+:;<=>[]{|}%&â€“~\\\"\")\r\n    df[\"char_word\"] = df.comment_text.map(char_to_word)\r\n    df.comment_text[df.char_word.notnull()] = df[df.char_word.notnull()].apply(\r\n        lambda x: word_regex(x, embeddings_index), axis=1)\r\n    df = df.drop([\"char_word\"], axis=1)\r\n    replace_dict = {\"'i'm\": \"i am\", \"'you're\": \"you are\", \"ain't\": \"is not\", \"aren't\": \"are not\",\r\n                    \"arn't\": \"are not\",\r\n                    \"c'mon\": \"common\",\r\n                    \"could'nt\": \"could not\", \"could've\": \"could have\", \"couldn't\": \"could not\",\r\n                    \"did'nt\": \"did not\",\r\n                    \"din't\": \"did not\",\r\n                    \"bullshet\": \"bullshit\",\r\n                    \"colour\": \"color\", \"centre\": \"center\", \"favourite\": \"favorite\",\r\n                    \"travelling\": \"traveling\", \"counselling\": \"counseling\", \"theatre\": \"theater\",\r\n                    \"cancelled\": \"canceled\", \"labour\": \"labor\", \"organisation\": \"organization\",\r\n                    \"narcisist\": \"narcissist\", \"qouta\": \"quota\", \"whst\": \"what\",\r\n                    \"demonetisation\": \"demonetization\",\r\n                    \"stooooooooooooooooooooopid\": \"stupid\", \"stoooooooooooopid\": \"stupid\",\r\n                    \"stooooooopid\": \"stupid\",\r\n                    \"stoooooopid\": \"stupid\",\r\n                    \"doens't\": \"do not\", \"dont't\": \"do not\", \"dosen't\": \"do not\",\r\n                    \"dosn't\": \"does not\",\r\n                    \"gov't\": \"government\",\r\n                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"havn't\": \"have not\",\r\n                    \"he'd\": \"he would\",\r\n                    \"he'd've\": \"he would have\",\r\n                    \"he'll\": \"he will\", \"here's\": \"here is\", \"how'd\": \"how did\",\r\n                    \"how'd'y\": \"how do you\",\r\n                    \"how'll\": \"how will\",\r\n                    \"how's\": \"how is\", \"i'am\": \"i am\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\r\n                    \"i'l\": \"i will\",\r\n                    \"i'll\": \"i will\",\r\n                    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i'ma\": \"i am\", \"i'v\": \"i have\",\r\n                    \"i've\": \"i have\",\r\n                    \"is'nt\": \"is not\",\r\n                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\r\n                    \"it'll've\": \"it will have\",\r\n                    \"let's\": \"let us\",\r\n                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\r\n                    \"mightn't\": \"might not\",\r\n                    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\r\n                    \"mustn't've\": \"must not have\",\r\n                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\r\n                    \"o'clock\": \"of the clock\",\r\n                    \"oughtn't\": \"ought not\",\r\n                    \"oughtn't've\": \"ought not have\", \"sha'n't\": \"shall not\", \"shan't\": \"shall not\",\r\n                    \"shan't've\": \"shall not have\",\r\n                    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\r\n                    \"she'll've\": \"she will have\",\r\n                    \"shoudn't\": \"should not\", \"should've\": \"should have\",\r\n                    \"shouldn't\": \"should not\",\r\n                    \"shouldn't've\": \"should not have\",\r\n                    \"so's\": \"so as\", \"so've\": \"so have\", \"that'd\": \"that would\",\r\n                    \"that'd've\": \"that would have\",\r\n                    \"that'll\": \"that will\",\r\n                    \"there'd\": \"there would\", \"there'd've\": \"there would have\",\r\n                    \"there'll\": \"there will\",\r\n                    \"there're\": \"there are\",\r\n                    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\r\n                    \"they'll've\": \"they will have\",\r\n                    \"they've\": \"they have\", \"this'll\": \"this will\", \"this's\": \"this is\",\r\n                    \"to've\": \"to have\",\r\n                    \"wan't\": \"want\",\r\n                    \"was'nt\": \"was not\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\r\n                    \"we'd've\": \"we would have\",\r\n                    \"we'll\": \"we will\",\r\n                    \"we'll've\": \"we will have\", \"we've\": \"we have\", \"weren't\": \"were not\",\r\n                    \"what'll\": \"what will\",\r\n                    \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\r\n                    \"what've\": \"what have\",\r\n                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\r\n                    \"where's\": \"where is\",\r\n                    \"where've\": \"where have\",\r\n                    \"who'd\": \"who would\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\r\n                    \"who're\": \"who are\",\r\n                    \"who's\": \"who is\",\r\n                    \"who've\": \"who have\", \"why'd\": \"why did\", \"why's\": \"why is\",\r\n                    \"why've\": \"why have\",\r\n                    \"will've\": \"will have\",\r\n                    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\r\n                    \"wouldn't've\": \"would not have\",\r\n                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\r\n                    \"y'all'd've\": \"you all would have\",\r\n                    \"y'all're\": \"you all are\",\r\n                    \"y'all've\": \"you all have\", \"y'know\": \"you know\", \"ya'll\": \"you all\",\r\n                    \"you'd\": \"you would\",\r\n                    \"you'd've\": \"you would have\", \"you'll've\": \"you will have\", \"you'r\": \"you are\",\r\n                    \"you've\": \"you have\",\r\n                    \"your'e\": \"you are\", \"your're\": \"you are\", \"motherf*cking\": \"mother fucking\"}\r\n    vocab0 = pd.Series(build_doc_vocab(df[df.target == 0].comment_text))\r\n    vocab1 = pd.Series(build_doc_vocab(df[df.target > 0].comment_text))\r\n    vocab = vocab1[(~vocab1.index.isin(vocab0[vocab0 > 25].index)) & (vocab1 > 20) & (\r\n        vocab1.index.str.lower().isin(embeddings_index)) & (~vocab1.index.str.contains(\"[^A-Za-z]\")) & (\r\n                       vocab1.index.str.len() > 3)]\r\n    vocab_all = pd.Series(build_doc_vocab(df.comment_text))\r\n    word_list = vocab_all[(~vocab_all.index.str.contains(\"[^A-Za-z\\^\\$\\*\\@\\#]\")) & (\r\n        vocab_all.index.str.contains(\"[A-Za-z]\")) & (vocab_all.index.str.contains(\"[\\^\\$\\*\\@\\#]\"))].index.values\r\n    word_list = [i for i in word_list if len(re.sub(\"[^a-zA-Z]\", \"\", i)) / len(i) >= 0.5]\r\n    word_dict = word_clean(word_list, vocab)\r\n    replace_dict = dict(replace_dict, **word_dict)\r\n    df[\"comment_text\"] = df.comment_text.map(lambda x: word_replace(x, replace_dict))\r\n    df = clean_punct(df, \"'*$^@#\")\r\n    del vocab, vocab0, vocab1\r\n    gc.collect()\r\n    longword_dict = long_word_dict_new(df, embeddings_index)\r\n    vocab = pd.Series(build_doc_vocab(df.comment_text))\r\n    vocab1 = vocab[~vocab.index.str.contains(\"[^a-zA-Z]\")]\r\n    vocab2 = vocab1[(vocab1 > 100) & (vocab1.index.isin(embeddings_index))]\r\n    vocab1 = vocab1.reset_index()\r\n    vocab1.columns = [\"word\", \"cnt\"]\r\n    vocab1[\"char_set\"] = vocab1.word.map(char_spell_check)\r\n    spell_dict = spell_dict_create(\r\n        vocab1[vocab1.char_set.notnull() & (~vocab1.word.str.lower().isin(embeddings_index))], vocab2)\r\n    replace_dict = dict(replace_dict, **longword_dict, **spell_dict)\r\n    del vocab, vocab1, vocab2\r\n    gc.collect()\r\n    df[\"comment_text\"] = df.comment_text.map(lambda x: word_replace(x, replace_dict))\r\n    if debug:\r\n        pass\r\n    else:\r\n        word_merge = word_merge_dict(df, embeddings_index)\r\n        \r\n        def bool_series_create(x):\r\n            \r\n            if sum([1 if key in x else 0 for key in word_merge.keys()]) > 0:\r\n                return True\r\n            else:\r\n                return False\r\n        \r\n        bool_series = df.comment_text.map(bool_series_create)\r\n        df.comment_text[bool_series] = df.comment_text[bool_series].map(lambda x: multiple_replace(x, word_merge))\r\n    train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\r\n    test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\r\n    if debug:\r\n        train = train.iloc[:10000]\r\n        test = test.iloc[:10000]\r\n    train[\"comment_text\"] = df[df.target.notnull()].comment_text.values\r\n    test[\"comment_text\"] = df[df.target.isnull()].comment_text.values\r\n    del embeddings_index, df\r\n    gc.collect()\r\n    return train, test\r\n\r\n\r\ndef token_fit(train, test):\r\n    identity_columns = ['asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual',\r\n                        'hindu',\r\n                        'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\r\n                        'muslim', 'other_disability',\r\n                        'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation',\r\n                        'physical_disability', 'psychiatric_or_mental_illness', 'transgender', 'white']\r\n    # Overall\r\n    weights = np.ones((len(train),)) / 4\r\n    # Subgroup\r\n    weights += (train[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int) / 4\r\n    # Background Positive, Subgroup Negative\r\n    weights += (((train['target'].values >= 0.5).astype(bool).astype(np.int) +\r\n                 (train[identity_columns].fillna(0).values < 0.5).sum(axis=1).astype(bool).astype(np.int)) > 1).astype(\r\n        bool).astype(np.int) / 4\r\n    # Background Negative, Subgroup Positive\r\n    weights += (((train['target'].values < 0.5).astype(bool).astype(np.int) +\r\n                 (train[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int)) > 1).astype(\r\n        bool).astype(np.int) / 4\r\n    loss_weight = 1.0 / weights.mean()\r\n    tok = Tokenizer(num_words=max_features, filters=\"\", lower=False)\r\n    tok.fit_on_texts(list(train.comment_text) + list(test.comment_text))\r\n    x_train = tok.texts_to_sequences(train.comment_text.values)\r\n    train_lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\r\n    x_train = torch.from_numpy(pad_sequences(x_train, maxlen=MAX_LEN))\r\n    x_test = tok.texts_to_sequences(test.comment_text.values)\r\n    test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\r\n    x_test = torch.from_numpy(pad_sequences(x_test, maxlen=MAX_LEN))\r\n    y_train = np.vstack([(train['target'].values >= 0.5).astype(np.int), weights]).T\r\n    y_train1 = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\r\n    y_train2 = np.vstack([(train.target.values >= i).astype(int) for i in [0.01, 0.25, 0.5, 0.75]])\r\n    y_train2 = np.vstack([y_train2, weights]).T\r\n    return x_train, x_test, y_train, y_train1, y_train2, tok, loss_weight, train_lengths, test_lengths\r\n\r\n\r\ndef get_coefs(word, *arr):\r\n    return word, np.asarray(arr, dtype='float32')\r\n\r\n\r\ndef load_embeddings1(path_list):\r\n    emb_idx = []\r\n    for path in path_list:\r\n        for line in open(path):\r\n            emb_idx.append(line.strip().split(\" \")[0].lower())\r\n    return list(set(emb_idx))\r\n\r\n\r\ndef load_embeddings(path):\r\n    with open(path) as f:\r\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\r\n\r\n\r\ndef build_embedding_matrix(tok, path):\r\n    word_docs = tok.word_docs\r\n    word_index = tok.word_index\r\n    embeddings_index = load_embeddings(path)\r\n    embedding_matrix = np.zeros((len(word_index) + 1, EMB_MAX_FEAT))\r\n    vocabs = {key: word_docs[key] for key in word_docs.keys() if word_docs[key] > 0 and key in embeddings_index}\r\n    alphabet = 'abcdefghijklmnopqrstuvwxyz'\r\n    \r\n    def edits1(word):\r\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\r\n        deletes = [a + b[1:] for a, b in splits if b]\r\n        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b) > 1]\r\n        replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]\r\n        inserts = [a + c + b for a, b in splits for c in alphabet]\r\n        return set(deletes + transposes + replaces + inserts)\r\n    \r\n    def known_edits2(word):\r\n        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in vocabs)\r\n    \r\n    def known(words):\r\n        return set(w for w in words if w in vocabs)\r\n    \r\n    def correct(word):\r\n        candidates = known([word]) or known(edits1(word)) or [word]\r\n        return max(candidates, key=vocabs.get)\r\n    \r\n    for key, i in word_index.items():\r\n        if i <= max_features:\r\n            word = key\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            word = key.lower()\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            word = key.upper()\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            word = key.capitalize()\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            word = ps.stem(key)\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            word = lc.stem(key)\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            word = sb.stem(key)\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            \r\n            if len(key) > 5:\r\n                word, embedding_vector = long_word(key, embeddings_index)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n            word = correct(key)\r\n            embedding_vector = embeddings_index.get(word)\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n                continue\r\n            if re.search(\"[a-zA-Z]\", key) and re.search(\"[^a-zA-Z]\", key):\r\n                key1 = re.sub(\"[^a-zA-Z]\", \"\", key)\r\n                word = key1\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n                word = key1.lower()\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n                word = key1.upper()\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n                word = key1.capitalize()\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n                word = ps.stem(key1)\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n                word = lc.stem(key1)\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n                word = correct(key1)\r\n                embedding_vector = embeddings_index.get(word)\r\n                if embedding_vector is not None:\r\n                    embedding_matrix[i] = embedding_vector\r\n                    continue\r\n            embedding_matrix[i] -= 1\r\n    del embeddings_index, vocabs\r\n    gc.collect()\r\n    return embedding_matrix.astype(np.float32)\r\n\r\n\r\ndef build_embeddings(tok):\r\n    embedding_matrix = np.concatenate(\r\n        [build_embedding_matrix(tok, f) for f in EMB_PATHS], axis=-1)\r\n    return embedding_matrix\r\n\r\n\r\ndef build_model(embedding_matrix, num_aux_targets, loss_weight):\r\n    '''\r\n    credits go to: https://www.kaggle.com/thousandvoices/simple-lstm/\r\n    '''\r\n    words = Input(shape=(MAX_LEN,))\r\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\r\n    x = SpatialDropout1D(0.3)(x)\r\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\r\n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\r\n    \r\n    hidden = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\r\n    result = Dense(3, activation='sigmoid')(hidden)\r\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\r\n    \r\n    model = Model(inputs=words, outputs=[result, aux_result])\r\n    model.compile(loss=[custom_loss, 'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\r\n    \r\n    return model\r\n\r\n\r\ndef submit(sub_preds, debug):\r\n    submission = pd.read_csv(os.path.join(JIGSAW_PATH, 'sample_submission.csv'), index_col='id')\r\n    if debug:\r\n        submission = submission.iloc[:10000]\r\n    submission['prediction'] = sub_preds\r\n    submission.reset_index(drop=False, inplace=True)\r\n    submission.to_csv('submission.csv', index=False)\r\n\r\n\r\ndef seed_everything(seed=1234):\r\n    random.seed(seed)\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n\r\nclass SpatialDropout(nn.Dropout2d):\r\n    def forward(self, x):\r\n        x = x.unsqueeze(2)  # (N, T, 1, K)\r\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\r\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\r\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\r\n        x = x.squeeze(2)  # (N, T, K)\r\n        return x\r\n\r\n\r\nclass SequenceBucketCollator():\r\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\r\n        self.choose_length = choose_length\r\n        self.sequence_index = sequence_index\r\n        self.length_index = length_index\r\n        self.label_index = label_index\r\n    \r\n    def __call__(self, batch):\r\n        batch = [torch.stack(x) for x in list(zip(*batch))]\r\n        \r\n        sequences = batch[self.sequence_index]\r\n        lengths = batch[self.length_index]\r\n        \r\n        length = self.choose_length(lengths)\r\n        mask = torch.arange(start=MAX_LEN, end=0, step=-1) < length\r\n        padded_sequences = sequences[:, mask]\r\n        \r\n        batch[self.sequence_index] = padded_sequences\r\n        \r\n        if self.label_index is not None:\r\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\r\n        \r\n        return batch\r\n\r\n\r\nclass NeuralNet(nn.Module):\r\n    def __init__(self, embedding_matrix, num_aux_targets, num_targets):\r\n        super(NeuralNet, self).__init__()\r\n        embed_size = embedding_matrix.shape[1]\r\n        \r\n        self.embedding = nn.Embedding(max_features, embed_size)\r\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\r\n        self.embedding.weight.requires_grad = False\r\n        self.embedding_dropout = SpatialDropout(0.3)\r\n        \r\n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\r\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\r\n        \r\n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\r\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\r\n        \r\n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, num_targets)\r\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\r\n    \r\n    def forward(self, x, lengths=None):\r\n        h_embedding = self.embedding(x.long())\r\n        h_embedding = self.embedding_dropout(h_embedding)\r\n        \r\n        h_lstm1, _ = self.lstm1(h_embedding)\r\n        h_lstm2, _ = self.lstm2(h_lstm1)\r\n        \r\n        # global average pooling\r\n        avg_pool = torch.mean(h_lstm2, 1)\r\n        # global max pooling\r\n        max_pool, _ = torch.max(h_lstm2, 1)\r\n        \r\n        h_conc = torch.cat((max_pool, avg_pool), 1)\r\n        h_conc_linear1 = F.relu(self.linear1(h_conc))\r\n        h_conc_linear2 = F.relu(self.linear2(h_conc))\r\n        \r\n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\r\n        \r\n        result = self.linear_out(hidden)\r\n        aux_result = self.linear_aux_out(hidden)\r\n        out = torch.cat([result, aux_result], 1)\r\n        \r\n        return out\r\n\r\n\r\ndef sigmoid(x):\r\n    return 1 / (1 + np.exp(-x))\r\n\r\n\r\ndef train_model(learn, test, output_dim, lr=0.001,\r\n                batch_size=512, n_epochs=5,\r\n                enable_checkpoint_ensemble=True):\r\n    all_test_preds = []\r\n    checkpoint_weights = [1,2,4,8,8]\r\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\r\n    n = len(learn.data.train_dl)\r\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6 ** (i)))) for i in range(n_epochs)]\r\n    sched = GeneralScheduler(learn, phases)\r\n    learn.callbacks.append(sched)\r\n    for epoch in range(n_epochs):\r\n        learn.fit(1)\r\n        test_preds = np.zeros((len(test), output_dim))\r\n        for i, x_batch in enumerate(test_loader):\r\n            X = x_batch[0].cuda()\r\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\r\n            test_preds[i * batch_size:(i + 1) * batch_size, :] = y_pred\r\n        all_test_preds.append(test_preds)\r\n    \r\n    if enable_checkpoint_ensemble:\r\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)\r\n    else:\r\n        test_preds = all_test_preds[-1]\r\n    return test_preds\r\n\r\n\r\ndef custom_loss(data, targets):\r\n    ''' Define custom loss function for weighted BCE on 'target' column '''\r\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 1:2])(data[:, :1], targets[:, :1])\r\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:, 1:], targets[:, 2:])\r\n    return (bce_loss_1 * loss_weight) + bce_loss_2\r\n\r\n\r\ndef custom_loss1(data, targets):\r\n    ''' Define custom loss function for weighted BCE on 'target' column '''\r\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 4:5])(data[:, :4], targets[:, :4])\r\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:, 4:], targets[:, 5:])\r\n    return (bce_loss_1 * loss_weight) + bce_loss_2\r\n\r\n\r\ndef train_(x_train, y_train, y_aux_train, x_test):\r\n    y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\r\n    test_dataset = data.TensorDataset(x_test, test_lengths)\r\n    train_dataset = data.TensorDataset(x_train, train_lengths, y_train_torch)\r\n    valid_dataset = data.Subset(train_dataset, indices=[0, 1])\r\n    del x_train, x_test\r\n    gc.collect()\r\n    \r\n    train_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(),\r\n                                            sequence_index=0,\r\n                                            length_index=1,\r\n                                            label_index=2)\r\n    \r\n    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\r\n    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\r\n    \r\n    databunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)\r\n    \r\n    del train_dataset, valid_dataset\r\n    gc.collect()\r\n    \r\n    for model_idx in range(NUM_MODELS):\r\n        all_test_preds = []\r\n        print('Model ', model_idx)\r\n        seed_everything(1234 + model_idx)\r\n        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1], y_train.shape[-1] - 1)\r\n        if y_train.shape[-1] > 2:\r\n            learn = Learner(databunch, model, loss_func=custom_loss1)\r\n        else:\r\n            learn = Learner(databunch, model, loss_func=custom_loss)\r\n        test_preds = train_model(learn, test_dataset, output_dim=y_train.shape[-1] + y_aux_train.shape[-1] - 1)\r\n        all_test_preds.append(test_preds)\r\n    preds = np.mean(all_test_preds, axis=0)\r\n    return preds\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    debug = False\r\n    train, test = load_data_and_clean(debug)\r\n    gc.collect()\r\n    \r\n    x_train, x_test, y_train, y_train1, y_train2, tok, loss_weight, train_lengths, test_lengths = token_fit(train, test)\r\n    max_features = min(max_features, len(tok.word_index) + 1)\r\n    preds1 = bert_predict(test, \"../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/\",\r\n                          \"../input/uncasedmodel1/\")\r\n    preds2 = bert_predict(test, \"../input/bert-pretrained-models/cased_l-12_h-768_a-12/cased_L-12_H-768_A-12/\",\r\n                          \"../input/casedmodel4/\")\r\n    del train, test\r\n    gc.collect()\r\n    if debug:\r\n        embedding_matrix = np.zeros((max_features, EMB_MAX_FEAT * 2))\r\n    else:\r\n        from nltk.stem import PorterStemmer\r\n        \r\n        ps = PorterStemmer()\r\n        from nltk.stem.lancaster import LancasterStemmer\r\n        \r\n        lc = LancasterStemmer()\r\n        from nltk.stem import SnowballStemmer\r\n        \r\n        sb = SnowballStemmer(\"english\")\r\n        embedding_matrix = build_embeddings(tok)\r\n        del sb, lc, ps\r\n        gc.collect()\r\n    \r\n    preds0 = train_(x_train, y_train2, y_train1, x_test)\r\n    sub_preds = preds0[:, 2] * 0.2 + preds1 * 0.48 + preds2 * 0.32\r\n    submit(sub_preds, debug)\r\n    submission = pd.read_csv(os.path.join(JIGSAW_PATH, 'sample_submission.csv'))\r\n    if debug:\r\n        submission = submission.iloc[:10000]\r\n    print(\"done\")","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}